{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pathlib\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "sns.set()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Babi',\n",
       " 'Calimerio',\n",
       " 'Chrysanthemum',\n",
       " 'Hydrangeas',\n",
       " 'Lisianthus',\n",
       " 'PingPong',\n",
       " 'Rosy',\n",
       " 'Tana']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flowers_dir = './Crop_img/'\n",
    "flower_labels = sorted(os.listdir(flowers_dir))\n",
    "flower_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "img_height = 299\n",
    "img_width = 299\n",
    "\n",
    "fig,ax = plt.subplots(2, 4, figsize=(20,10))\n",
    "ax=ax.flatten()\n",
    "\n",
    "flowers_dir_path_object = pathlib.Path(flowers_dir)\n",
    "\n",
    "for label_number,flower in enumerate(flower_labels):\n",
    "    # get all paths of images inside flower subdirectory\n",
    "    flower_images_paths = list(flowers_dir_path_object.glob(f'{flower}/*.jpg'))\n",
    "    ax[label_number].set_title(flower.capitalize())\n",
    "    ax[label_number].axis('off')\n",
    "    flower_img = np.array(load_img(flower_images_paths[0].__str__()))\n",
    "    ax[label_number].matshow(flower_img)\n",
    "    # print(flower_images_paths[0].__str__())\n",
    "    for image in flower_images_paths:\n",
    "        # load and resize image\n",
    "        resized_image = load_img(str(image),target_size=(img_height, img_width))\n",
    "        # append features in X and labels in y\n",
    "        X.append(np.array(resized_image))\n",
    "        y.append(label_number)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scanData():\n",
    "    data_count = pd.DataFrame(data={\"flower\": [], \"count\": []})\n",
    "    meta_data_df = pd.DataFrame(data={\"image_path\": [], \"flower_type\": []})\n",
    "    folder_path = flowers_dir\n",
    "\n",
    "    subdirectories = next(os.walk(folder_path))[1]\n",
    "\n",
    "    print(subdirectories)\n",
    "\n",
    "    for i in subdirectories:\n",
    "        sub = next(os.walk(folder_path+\"/\"+i))\n",
    "        \n",
    "        new_meta_data = pd.DataFrame(data={\"image_path\": [ (sub[0] + \"/\" + j) for j in sub[2]] , \"flower_type\": i})\n",
    "        meta_data_df = pd.concat([meta_data_df, new_meta_data], ignore_index=True)\n",
    "        \n",
    "        new_data = pd.DataFrame(data={\"flower\": i, \"count\": len(sub[2])}, index=[0])      \n",
    "        data_count = pd.concat([data_count, new_data], ignore_index=True)\n",
    "        print(i, len(sub[2]), len([ sub[0] + j for j in sub[2]]))\n",
    "\n",
    "    return data_count, meta_data_df\n",
    "\n",
    "img_count, meta_data = scanData()\n",
    "img_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.barplot(data=img_count, x='flower', y='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(new_image = tf.image.per_image_standardization(mpimg.imread(meta_data['image_path'][i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import random\n",
    "import pathlib\n",
    "\n",
    "fig, ax = plt.subplots(2, 4, figsize=(20,20), sharex=False)\n",
    "ax = ax.flatten()\n",
    "\n",
    "flowers_dir = './Flowers_cleaned/'\n",
    "flower_labels = sorted(os.listdir(flowers_dir))\n",
    "flower_labels\n",
    "flowers_dir_path_object = pathlib.Path(flowers_dir)\n",
    "for label_number,flower in enumerate(flower_labels):\n",
    "    \n",
    "    # get all paths of images inside flower subdirectory\n",
    "    flower_images_paths = list(flowers_dir_path_object.glob(f'{flower}/*.jpg'))\n",
    "    \n",
    "    ax[label_number].set_title(flower.capitalize())\n",
    "    ax[label_number].axis('off')\n",
    "    flower_img = np.array(load_img(flower_images_paths[0].__str__()))\n",
    "    ax[label_number].matshow(flower_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data):\n",
    "    TEST_RATIO = 0.2\n",
    "    train_data, test_data = train_test_split(data, test_size=TEST_RATIO, random_state=100)\n",
    "    # train_data, val_data = train_test_split(train_data, test_size=TEST_RATIO, random_state=100)\n",
    "\n",
    "    print(\"Train data : {}, Test Data: {}\".format(\n",
    "        train_data.shape[0], test_data.shape[0]))\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = split_dataset(meta_data)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.histplot(data=test_data, x='flower_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (10572, 299, 299, 3)\n",
      "y shape: (10572,)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print('X shape: {}'.format(X.shape))\n",
    "print('y shape: {}'.format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape: (7929, 299, 299, 3)\n",
      "X test shape: (2643, 299, 299, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0)\n",
    "\n",
    "print('X train shape: {}'.format(X_train.shape))\n",
    "print('X test shape: {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(train_loss, val_loss, train_metric, val_metric, metric_name='Accuracy'):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_loss, 'r--')\n",
    "    plt.plot(val_loss, 'b--')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_metric, 'r--')\n",
    "    plt.plot(val_metric, 'b--')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    \n",
    "    plt.axhline(y = max(train_metric), color = 'r', linestyle = '-')\n",
    "    plt.axhline(y = max(val_metric), color = 'b', linestyle = '-')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPooling2D, Add, Flatten, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def identity_block(X, filters):\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid')(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F2, kernel_size=(3, 3), strides=(1, 1), padding='same')(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid')(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "def convolutional_block(X, filters, s=2):\n",
    "    F1, F2, F3 = filters\n",
    "\n",
    "    X_shortcut = X\n",
    "\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid')(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F2, kernel_size=(3, 3), strides=(1, 1), padding='same')(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid')(X)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "\n",
    "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid')(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3)(X_shortcut)\n",
    "\n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "def ResNet50(input_shape=(224, 224, 3), classes=1000):\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    X = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), padding='same')(X_input)\n",
    "    X = BatchNormalization(axis=3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    X = convolutional_block(X, filters=[64, 64, 256], s=1)\n",
    "    X = identity_block(X, filters=[64, 64, 256])\n",
    "    X = identity_block(X, filters=[64, 64, 256])\n",
    "\n",
    "    X = convolutional_block(X, filters=[128, 128, 512], s=2)\n",
    "    X = identity_block(X, filters=[128, 128, 512])\n",
    "    X = identity_block(X, filters=[128, 128, 512])\n",
    "    X = identity_block(X, filters=[128, 128, 512])\n",
    "\n",
    "    X = convolutional_block(X, filters=[256, 256, 1024], s=2)\n",
    "    X\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(input_shape=(img_height, img_width, 3), classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'compile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m sgd_opt \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mSGD(\n\u001b[0;32m      2\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m,\n\u001b[0;32m      3\u001b[0m     momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m,\n\u001b[0;32m      4\u001b[0m )\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m( optimizer\u001b[38;5;241m=\u001b[39msgd_opt,loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'compile'"
     ]
    }
   ],
   "source": [
    "sgd_opt = tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.0001,\n",
    "    momentum=0.8,\n",
    ")\n",
    "model.compile( optimizer=sgd_opt,loss='sparse_categorical_crossentropy',metrics='accuracy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(train_generator, validation_data = test_generator,\n",
    "#                               epochs=20, verbose=2, class_weight=class_weights_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(history.history['loss'], history.history['val_loss'], \n",
    "                    history.history['accuracy'], history.history['val_accuracy'], \n",
    "                    metric_name='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm = sklearn.metrics.multilabel_confusion_matrix(test_generator.classes, model.predict(test_generator))\n",
    "prediction = model.predict(test_generator)\n",
    "index_max = prediction.argmax(axis=1)\n",
    "print(index_max.shape)\n",
    "# print(prediction[index_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fix, ax = plt.subplots(2, 4, sharex=True, figsize=(20,10))\n",
    "# ax = ax.flatten()\n",
    "# for i in range(7):\n",
    "    # sns.histplot(data=new, ax=ax[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = tf.math.confusion_matrix(test_generator.classes, index_max, num_classes=8)\n",
    "# cm = sklearn.metrics.multilabel_confusion_matrix(test_generator.classes, index_max)\n",
    "\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "fx=sns.heatmap(cm, annot=True, fmt=\".2f\",cmap=\"GnBu\")\n",
    "fx.set_title('Confusion Matrix \\n');\n",
    "fx.set_xlabel('\\n Predicted Values\\n')\n",
    "fx.set_ylabel('Actual Values\\n');\n",
    "fx.xaxis.set_ticklabels(train_generator.class_indices)\n",
    "fx.yaxis.set_ticklabels(train_generator.class_indices)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
